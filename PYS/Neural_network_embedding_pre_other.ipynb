{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 200)\n",
    "from pathlib import Path\n",
    "\n",
    "from plotnine import *\n",
    "import os\n",
    "# suppress tf informational and warning messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, Sequential\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "RANDOM_STATE = 2112\n",
    "keras.utils.set_random_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotnine\n",
      "  Downloading plotnine-0.12.4-py3-none-any.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: matplotlib>=3.6.0 in /opt/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from plotnine) (3.7.5)\n",
      "Collecting mizani<0.10.0,>0.9.0 (from plotnine)\n",
      "  Downloading mizani-0.9.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /opt/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from plotnine) (1.24.3)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /opt/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from plotnine) (2.0.3)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /opt/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from plotnine) (0.5.6)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from plotnine) (1.10.1)\n",
      "Requirement already satisfied: statsmodels>=0.14.0 in /opt/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from plotnine) (0.14.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from matplotlib>=3.6.0->plotnine) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from matplotlib>=3.6.0->plotnine) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from matplotlib>=3.6.0->plotnine) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from matplotlib>=3.6.0->plotnine) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from matplotlib>=3.6.0->plotnine) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from matplotlib>=3.6.0->plotnine) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from matplotlib>=3.6.0->plotnine) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from matplotlib>=3.6.0->plotnine) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /opt/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from matplotlib>=3.6.0->plotnine) (6.4.0)\n",
      "Collecting backports.zoneinfo (from mizani<0.10.0,>0.9.0->plotnine)\n",
      "  Using cached backports.zoneinfo-0.2.1-cp38-cp38-macosx_11_0_arm64.whl\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from pandas>=1.5.0->plotnine) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from pandas>=1.5.0->plotnine) (2024.1)\n",
      "Requirement already satisfied: six in /opt/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from patsy>=0.5.1->plotnine) (1.16.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib>=3.6.0->plotnine) (3.17.0)\n",
      "Downloading plotnine-0.12.4-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading mizani-0.9.3-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: backports.zoneinfo, mizani, plotnine\n",
      "Successfully installed backports.zoneinfo-0.2.1 mizani-0.9.3 plotnine-0.12.4\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIR = Path('/kaggle/input/playground-series-s4e7')\n",
    "TARGET_NAME = 'Response'\n",
    "\n",
    "train_data = pd.read_csv('./train.csv')\n",
    "test_data = pd.read_csv('./test.csv')\n",
    "\n",
    "\n",
    "cont_features = []    \n",
    "cat_features = [\n",
    "    'Gender', 'Driving_License', 'Previously_Insured', \n",
    "    'Vehicle_Age', 'Vehicle_Damage', \n",
    "     'Age', 'Vintage', 'Annual_Premium']\n",
    "\n",
    "def fe(df):\n",
    "    return df\n",
    "\n",
    "# convert to pipeline:\n",
    "all_data = fe(pd.concat([test_data, train_data]))\n",
    "\n",
    "oe = preprocessing.OrdinalEncoder()\n",
    "all_data[cat_features] = oe.fit_transform(all_data.filter(cat_features)).astype('int')\n",
    "\n",
    "cat_features_card = {}\n",
    "for f in cat_features:\n",
    "    cat_features_card[f] = 1 + all_data[f].max()\n",
    "\n",
    "\n",
    "# treat the 0-1 features as continuous\n",
    "# everything else will go into an embedding layer.\n",
    "cont_features = [\n",
    "    'Gender', 'Driving_License', 'Previously_Insured', 'Vehicle_Damage']\n",
    "cat_features = [ \n",
    "    'Vehicle_Age', 'Policy_Sales_Channel', \n",
    "    'Region_Code', 'Age', 'Vintage', 'Annual_Premium']\n",
    "\n",
    "train_data = all_data.query(f\"not {TARGET_NAME}.isna()\")\n",
    "test_data  = all_data.query(f\"{TARGET_NAME}.isna()\").drop(columns=[TARGET_NAME])\n",
    "features = cont_features + cat_features\n",
    "\n",
    "# tidy up\n",
    "all_data = None\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Region_code_preprocessing(df):\n",
    "    if df < 26:\n",
    "        return 0\n",
    "    elif 26<= df <29:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "def policy_code_preprocsssing(df):\n",
    "    if df < 124:\n",
    "        return 0\n",
    "    elif df < 152:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "train_data['Region_Code'] = train_data['Region_Code'].apply(Region_code_preprocessing)\n",
    "train_data['Policy_Sales_Channel'] = train_data['Policy_Sales_Channel'].apply(policy_code_preprocsssing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(cat_features, cont_features):\n",
    "\n",
    "    # Define input layers\n",
    "    cat_inputs = [layers.Input(shape=(1,), name=f'cat{i}') for i in range(len(cat_features))]\n",
    "    cont_inputs = layers.Input(shape=(len(cont_features),))\n",
    "    #cont_inputs_bn = layers.BatchNormalization()(cont_inputs)\n",
    "                                \n",
    "    # Embedding layers for categorical inputs\n",
    "    flat_embeddings = []\n",
    "    for i, f in enumerate(cat_features):\n",
    "        input_dim = int(cat_features_card[f])\n",
    "        output_dim = int(min(64, round(1.6 * input_dim ** .56))) # based on the fastai library\n",
    "        embedding = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)(cat_inputs[i])\n",
    "        embedding = layers.SpatialDropout1D(.3)(embedding)\n",
    "        flat_embeddings.append(layers.Flatten()(embedding))\n",
    "                                \n",
    "    concatenated_inputs = layers.Concatenate()(flat_embeddings + [cont_inputs, ])\n",
    "    concatenated_inputs_bn = layers.BatchNormalization()(concatenated_inputs)\n",
    "\n",
    "    x = layers.Dense(256, activation='mish')(concatenated_inputs_bn)\n",
    "    #x = layers.Dropout(.3)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    for units in (128,):\n",
    "        inp = layers.Concatenate()([x, concatenated_inputs_bn])\n",
    "        x = layers.Dense(units=units, activation='mish')(inp)\n",
    "        x = layers.Dropout(.3)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # output layer\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    return keras.Model(cat_inputs + [cont_inputs], outputs)\n",
    "\n",
    "# cosine decay - for later.\n",
    "# initially use a single learning rate\n",
    "# for a small number of epochs\n",
    "epochs = 4\n",
    "callbacks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_logloss(y, preds):\n",
    "    return metrics.log_loss(y, preds)\n",
    "\n",
    "def fold_auc(y, preds):\n",
    "    return metrics.roc_auc_score(y, preds)\n",
    "\n",
    "# to feed data into the NN\n",
    "# we feed the categoricals column by column,\n",
    "# and the continuous features in one lump.\n",
    "cat_idxs= []\n",
    "cont_idxs = []\n",
    "for f in cat_features:\n",
    "    cat_idxs.append([features.index(f)])\n",
    "for f in cont_features:\n",
    "    cont_idxs.append(features.index(f))\n",
    "    \n",
    "feature_idxs = cat_idxs + [cont_idxs]\n",
    "\n",
    "def to_nn_feed(df):\n",
    "    X = df[feats].values\n",
    "    result = []\n",
    "    for f_idx in feature_idxs:\n",
    "        # housekeeping: to feed data into the NN\n",
    "        # we feed the categoricals column by column,\n",
    "        # and the continuous features in one lump.\n",
    "        result.append(X[:, f_idx])\n",
    "    return result\n",
    "\n",
    "def fit_fold(tr, vl, ts):\n",
    "\n",
    "    model = build_model(cat_features, cont_features)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.AdamW(learning_rate=1E-4),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['auc'])\n",
    "\n",
    "    history = model.fit(\n",
    "          to_nn_feed(tr), tr[TARGET_NAME],\n",
    "          validation_data=(to_nn_feed(vl), vl[TARGET_NAME]),\n",
    "          batch_size=BS,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks,\n",
    "          verbose=0\n",
    "    )\n",
    "\n",
    "    vl_pred = model.predict(to_nn_feed(vl), verbose=0, batch_size=BS).flatten()\n",
    "    ts_pred = model.predict(to_nn_feed(ts), verbose=0, batch_size=BS).flatten()\n",
    "    \n",
    "    vl_metric = fold_auc(vl[TARGET_NAME], vl_pred)\n",
    "    return vl_pred, ts_pred, vl_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 5\n",
    "BS = 3000\n",
    "feats = features\n",
    "\n",
    "vl_preds = np.zeros(len(train_data))\n",
    "ts_preds = np.zeros(len(test_data))\n",
    "vl_metrics = []\n",
    "\n",
    "keras.utils.set_random_seed(RANDOM_STATE)\n",
    "k_fold = StratifiedKFold(n_splits=N_FOLDS, random_state=RANDOM_STATE, shuffle=True)\n",
    "for tr_idx, vl_idx in k_fold.split(train_data, train_data[TARGET_NAME]):\n",
    "    tr = train_data.loc[tr_idx]\n",
    "    vl = train_data.loc[vl_idx]\n",
    "\n",
    "    vl_pred, ts_pred, vl_metric = fit_fold(tr, vl, test_data)\n",
    "    \n",
    "    print(f'  -- fold auc {vl_metric:2.6f}')\n",
    "    vl_metrics.append(vl_metric)\n",
    "    vl_preds[vl_idx] += vl_pred\n",
    "    ts_preds += ts_pred / N_FOLDS\n",
    "\n",
    "# overall metric:\n",
    "vl_metric = fold_auc(\n",
    "    train_data[TARGET_NAME], vl_preds)\n",
    "print(f'  ----------- {vl_metric:2.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.19",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee7d7838ef53998fd22ad7449b76e48b4013ea11e59d28ee193f2cd757746339"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
